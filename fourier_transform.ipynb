{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Decoding Financial Market Oscillations: A Fourier and Wavelet Transform Analysis\n",
        "## apoth3osis R&D\n",
        "\n",
        "Welcome to this advanced analytics notebook from apoth3osis R&D. In the unpredictable realm of financial markets, identifying underlying patterns is key to developing robust algorithmic trading strategies. This notebook explores the power of Fourier and Wavelet Transforms to uncover hidden oscillatory behaviors within minute-level EUR/USD exchange rate data. By transforming price movements into their constituent frequencies, we aim to isolate significant cyclical components and understand how they contribute to overall market dynamics. This foundational research seeks to determine if sufficient signal exists within the data to justify the application of advanced machine learning techniques, including those leveraging Fourier and Wavelet analysis for predictive modeling and investment opportunities."
      ],
      "metadata": {
        "id": "a3TrL3kcx_o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Environment Setup and Data Ingestion\n",
        "\n",
        "This section prepares the Python environment by importing necessary libraries and provides a mechanism to securely upload the financial time series data. We will use `pandas` for data manipulation, `numpy` for numerical operations, `matplotlib` for visualization, `scipy.fftpack` for Fourier Transforms, `scipy.signal` for signal processing, `statsmodels` for time series analysis, and `pywt` for Wavelet Transforms. A robust file upload method is implemented to ensure data privacy and ease of use within the Colab environment."
      ],
      "metadata": {
        "id": "hQqdWb11mgog"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Consolidate all library imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "from scipy.fftpack import fft, ifft, fftfreq\n",
        "from scipy import signal\n",
        "from statsmodels.tsa.stattools import acf\n",
        "from statsmodels.graphics.tsaplots import plot_acf\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pywt\n",
        "from google.colab import files, drive\n",
        "from mpl_toolkits.mplot3d import Axes3D # For Bloch Sphere visualization\n",
        "import os\n",
        "from multiprocessing import Pool, cpu_count\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from datetime import timedelta\n",
        "\n",
        "# File upload mechanism\n",
        "def upload_data():\n",
        "    uploaded = files.upload()\n",
        "    for fn in uploaded.keys():\n",
        "        print(f'User uploaded file \"{fn}\"')\n",
        "        return fn # Assuming single file upload\n",
        "    return None\n",
        "\n",
        "file_name = None\n",
        "try:\n",
        "    file_name = upload_data()\n",
        "    if file_name:\n",
        "        df = pd.read_csv(f'/content/{file_name}')\n",
        "        df['Date'] = pd.to_datetime(df['Date'])\n",
        "        df.set_index('Date', inplace=True)\n",
        "        print(\"\\nData loaded successfully. Displaying head and info:\")\n",
        "        print(df.head())\n",
        "        print(df.info())\n",
        "    else:\n",
        "        print(\"No file uploaded. Please upload the 'complete_interpolated_eur_usd_data.csv' file.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file {file_name} was not found. Please ensure it is correctly uploaded.\")\n",
        "except pd.errors.EmptyDataError:\n",
        "    print(f\"Error: The file {file_name} is empty.\")\n",
        "except pd.errors.ParserError:\n",
        "    print(f\"Error: Could not parse {file_name}. Please check the CSV format.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "# Ensure the 'EUR/USD' column is available and resample if data is loaded\n",
        "df_resampled = pd.DataFrame() # Initialize empty DataFrame\n",
        "if 'df' in locals() and not df.empty:\n",
        "    if 'EUR/USD' in df.columns:\n",
        "        # Resample the data to a uniform time interval (e.g., 1 minute)\n",
        "        # This is crucial for consistent FFT analysis, which assumes uniformly sampled data.\n",
        "        df_resampled = df.resample('1Min').mean().dropna().reset_index() # Drop NaNs introduced by resampling\n",
        "        print(\"\\nData resampled to 1-minute intervals and NaNs dropped. Displaying head:\")\n",
        "        print(df_resampled.head())\n",
        "    else:\n",
        "        print(\"Error: 'EUR/USD' column not found in the uploaded data.\")\n",
        "else:\n",
        "    print(\"DataFrame 'df' is not loaded or is empty. Cannot proceed with resampling.\")\n"
      ],
      "metadata": {
        "id": "tYfqE67mGElZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Charting the Original Signal\n",
        "\n",
        "Visualizing the raw EUR/USD exchange rate data is the first step in understanding its behavior. This chart provides a macroscopic view of the signal over time, highlighting major trends, volatility shifts, and any apparent long-term patterns. It serves as a baseline for comparing subsequent transformations and analyses."
      ],
      "metadata": {
        "id": "vFHVNB44GEla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not df_resampled.empty:\n",
        "    try:\n",
        "        # Visualize the data\n",
        "        plt.figure(figsize=(15, 7))\n",
        "        plt.plot(df_resampled['Date'], df_resampled['EUR/USD'], color='blue', linewidth=0.8)\n",
        "        plt.xlabel('Date', fontsize=12)\n",
        "        plt.ylabel('EUR/USD Price', fontsize=12)\n",
        "        plt.title('Forex Pairing Price Chart (1-Minute Resampled Data)', fontsize=14)\n",
        "        plt.grid(True, linestyle=':', alpha=0.6)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('/content/forex_price_chart.png')\n",
        "        plt.show()\n",
        "        print(\"Forex price chart saved as 'forex_price_chart.png'\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error charting original signal: {e}\")\n",
        "else:\n",
        "    print(\"Cannot chart original signal as 'df_resampled' is empty.\")\n"
      ],
      "metadata": {
        "id": "rIEjKVEjbXuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Fourier Transform Analysis: Decomposing the Signal\n",
        "\n",
        "The Fourier Transform (FT) is a powerful mathematical tool that decomposes a time-domain signal into its constituent frequencies. This allows us to understand the various cycles and periodicities present in the data. For financial data, this can reveal underlying market rhythms that might not be obvious in the raw price series. The output of the FT provides information on the amplitude (strength) and phase (starting point) of each frequency component.\n",
        "\n",
        "**Why it's useful:** The FT helps us identify dominant cycles that could be driving price movements. For instance, a strong amplitude at a particular frequency might indicate a recurring pattern that could be leveraged in trading strategies. The DC component (zero frequency) represents the average value or overall trend of the signal. In this analysis, we will specifically remove the DC component to focus on the oscillatory behavior around the trend, as the user is interested in predictable patterns of otherwise unpredictable data."
      ],
      "metadata": {
        "id": "KPUU41Zrc6wY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_fourier_transform(data_series):\n",
        "    \"\"\"Performs Fourier Transform on a given time series.\n",
        "\n",
        "    Args:\n",
        "        data_series (pd.Series): The time series data (e.g., 'EUR/USD' prices).\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame containing Frequency, Amplitude, Phase, Time, Price, and Date/Time.\n",
        "        np.ndarray: The raw FFT values.\n",
        "    \"\"\"\n",
        "    n = len(data_series)\n",
        "    t = np.arange(n)  # Time array in samples\n",
        "    X = fft(data_series.values)  # Fourier transform\n",
        "\n",
        "    # Calculate the frequency array. 'd' is the sample spacing.\n",
        "    # For 1-minute resampling, d=1 minute. To convert to Hz, if data is daily, d=1 day.\n",
        "    # Given this is minute data, we can define frequency in cycles per minute or adjust 'd' to seconds/hours/days.\n",
        "    # Assuming 'd' is 1 minute for now, so frequencies are in cycles/minute.\n",
        "    freq = fftfreq(n, d=1)\n",
        "\n",
        "    amplitude = np.abs(X)\n",
        "    phase = np.angle(X)\n",
        "\n",
        "    ft_output = pd.DataFrame({\n",
        "        'Frequency': freq,\n",
        "        'Amplitude': amplitude,\n",
        "        'Phase': phase,\n",
        "        'Time': t,\n",
        "        'Price': data_series.values,\n",
        "        'Date/Time': data_series.index # Use the original datetime index\n",
        "    })\n",
        "    return ft_output, X\n",
        "\n",
        "if not df_resampled.empty and 'EUR/USD' in df_resampled.columns:\n",
        "    try:\n",
        "        # Set 'Date' as index for the series before passing to FT function\n",
        "        ft_output_df, full_fft_values = perform_fourier_transform(df_resampled.set_index('Date')['EUR/USD'])\n",
        "\n",
        "        # Save the output to a new CSV file\n",
        "        output_path = '/content/ft_output.csv'\n",
        "        ft_output_df.to_csv(output_path, index=False)\n",
        "        print(f\"Fourier Transform output saved to '{output_path}'\")\n",
        "        print(\"\\nFirst 5 rows of Fourier Transform Output DataFrame:\")\n",
        "        print(ft_output_df.head())\n",
        "    except Exception as e:\n",
        "        print(f\"Error performing Fourier Transform: {e}\")\n",
        "else:\n",
        "    print(\"Cannot perform Fourier Transform as 'df_resampled' is empty or 'EUR/USD' column is missing.\")\n"
      ],
      "metadata": {
        "id": "AfNVwBCmc9uK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Visualizing Fourier Transform Components on a Bloch Sphere\n",
        "\n",
        "A Bloch Sphere is a geometric representation of the pure state space of a two-level quantum mechanical system. While typically used in quantum computing to visualize qubits, we adapt it here conceptually to represent the complex nature of our Fourier Transform components (frequency, amplitude, and phase). By mapping these properties to a 3D spherical coordinate system, we can gain a novel visual perspective on the relationships between different oscillatory patterns in the financial data. The frequency can be mapped to the radius, and the phase can be mapped to an angular coordinate (theta).\n",
        "\n",
        "**Why it's useful:** This unconventional visualization helps to intuitively grasp how different frequency components, with their respective strengths and timings, contribute to the overall signal. In the long term, this conceptualization could pave the way for leveraging quantum computing principles to analyze complex financial data, where Bloch spheres might represent entire quantum functions or states related to market dynamics."
      ],
      "metadata": {
        "id": "yScoZmaIewQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'ft_output_df' in locals() and not ft_output_df.empty:\n",
        "    try:\n",
        "        # Take the first 30 rows (or fewer if less data is available) for visualization\n",
        "        ft_output_30 = ft_output_df.iloc[0:min(30, len(ft_output_df))].copy()\n",
        "\n",
        "        # For visualization on a sphere, we'll map frequency to radius and phase to one of the angles.\n",
        "        # We normalize the frequency to avoid extremely large spheres and fix phi for a clearer view of phase variations.\n",
        "        r_max = ft_output_30['Frequency'].abs().max() # Max absolute frequency for scaling\n",
        "        if r_max == 0: # Handle case where all frequencies are zero\n",
        "            print(\"All frequencies are zero, cannot create Bloch Sphere visualization.\")\n",
        "        else:\n",
        "            r = ft_output_30['Frequency'].abs() / r_max  # Scale frequency to a reasonable radius (0 to 1)\n",
        "            theta = ft_output_30['Phase']  # Phase in radians\n",
        "            phi = np.pi / 2  # Fixed angle for visualization, effectively plotting on the XY plane for clarity\n",
        "\n",
        "            # Convert spherical to Cartesian coordinates\n",
        "            # x = r * sin(phi) * cos(theta)\n",
        "            # y = r * sin(phi) * sin(theta)\n",
        "            # z = r * cos(phi)\n",
        "            # Since phi is fixed at pi/2, sin(phi)=1 and cos(phi)=0, so z will be 0.\n",
        "            x = r * np.cos(theta)\n",
        "            y = r * np.sin(theta)\n",
        "            z = np.zeros_like(r) # All points will be on the XY plane due to phi = pi/2\n",
        "\n",
        "            # Create 3D plot for the vectors on a sphere\n",
        "            fig = plt.figure(figsize=(10, 8))\n",
        "            ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "            # Plot vectors from the origin\n",
        "            for i in range(len(ft_output_30)):\n",
        "                ax.quiver(0, 0, 0, x[i], y[i], z[i], color='b', arrow_length_ratio=0.1)\n",
        "\n",
        "            # Add a sphere for context (optional, but good for visualization)\n",
        "            u = np.linspace(0, 2 * np.pi, 100)\n",
        "            v = np.linspace(0, np.pi, 100)\n",
        "            sphere_x = np.outer(np.cos(u), np.sin(v))\n",
        "            sphere_y = np.outer(np.sin(u), np.sin(v))\n",
        "            sphere_z = np.outer(np.ones(np.size(u)), np.cos(v))\n",
        "            ax.plot_surface(sphere_x, sphere_y, sphere_z, color='c', alpha=0.1, linewidth=0)\n",
        "\n",
        "            # Set labels\n",
        "            ax.set_xlabel('X (Scaled Frequency Cosine Phase)')\n",
        "            ax.set_ylabel('Y (Scaled Frequency Sine Phase)')\n",
        "            ax.set_zlabel('Z (Fixed)')\n",
        "\n",
        "            # Set the limits to be symmetric around zero for a centered sphere\n",
        "            max_range = 1.0 # Since radius is scaled to 0-1\n",
        "            ax.set_xlim([-max_range, max_range])\n",
        "            ax.set_ylim([-max_range, max_range])\n",
        "            ax.set_zlim([-max_range, max_range])\n",
        "\n",
        "            # Set the title\n",
        "            ax.set_title('Fourier Transform Components on a Conceptual Sphere (First 30 Waves)')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig('/content/bloch_sphere_representation.png')\n",
        "            plt.show()\n",
        "            print(\"Bloch Sphere representation plot saved as 'bloch_sphere_representation.png'\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating Bloch Sphere representation: {e}\")\n",
        "else:\n",
        "    print(\"Cannot generate Bloch Sphere representation as 'ft_output_df' is empty.\")\n"
      ],
      "metadata": {
        "id": "NmOV10t2e32c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Segmenting and Analyzing the Signal for Time-Dependent Patterns\n",
        "\n",
        "To understand how the signal's frequency content evolves over time, we divide the entire dataset into multiple segments (e.g., 20 equal parts). Performing a Fourier Transform on each segment allows us to capture time-dependent elements and observe local variations in the dominant frequencies, amplitudes, and phases. This approach helps in identifying shifts in market behavior that might not be apparent from a single, global Fourier Transform.\n",
        "\n",
        "**Why it's useful:** Financial markets are dynamic, and patterns can emerge or disappear over time. By segmenting the data, we gain a more granular view, which is essential for developing adaptive trading strategies. We expect to see different dominant frequencies and magnitudes in different segments, reflecting the non-stationary nature of financial time series."
      ],
      "metadata": {
        "id": "UBxzrWRBkwkh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def numerical_ft_segment(data_array):\n",
        "    \"\"\"Performs Fourier Transform on a single segment of data.\n",
        "\n",
        "    Args:\n",
        "        data_array (np.ndarray): The segment of time series data.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame containing Frequency, Amplitude, and Phase for the segment.\n",
        "    \"\"\"\n",
        "    n = len(data_array)\n",
        "    F_values = fft(data_array)\n",
        "    freq = fftfreq(n, d=1)  # Assuming 1-minute sample spacing\n",
        "    return pd.DataFrame({\n",
        "        'Frequency': freq,\n",
        "        'Amplitude': np.abs(F_values),\n",
        "        'Phase': np.angle(F_values)\n",
        "    })\n",
        "\n",
        "def process_data_segment(args):\n",
        "    \"\"\"Function to process a single segment of data for parallel execution.\n",
        "\n",
        "    Args:\n",
        "        args (tuple): A tuple containing start index, end index, and segment number.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Fourier Transform results for the segment with a 'Segment' column.\n",
        "    \"\"\"\n",
        "    start, end, segment_num, df_data = args\n",
        "    segment_data = df_data['EUR/USD'].iloc[start:end]\n",
        "    ft_segment = numerical_ft_segment(segment_data.values)\n",
        "    ft_segment['Segment'] = segment_num\n",
        "    return ft_segment\n",
        "\n",
        "if not df_resampled.empty:\n",
        "    try:\n",
        "        # Perform initial FT on entire dataset\n",
        "        # This output will serve as the optimal basis waves for comparison later.\n",
        "        print(\"Performing FT on entire dataset to establish optimal basis waves...\")\n",
        "        ft_full_optimal, _ = perform_fourier_transform(df_resampled.set_index('Date')['EUR/USD'])\n",
        "\n",
        "        # Combine with original data (for completeness, though not directly used for correlation here)\n",
        "        # The `ft_full_optimal` DataFrame already contains 'Frequency', 'Amplitude', 'Phase'\n",
        "        # from the full dataset. We need to be careful with column names in final output.\n",
        "\n",
        "        # Prepare segments for parallel processing\n",
        "        num_segments = 20 # Chosen to allow for more time dependent elements to be extracted\n",
        "        segment_size = len(df_resampled) // num_segments\n",
        "        segments_args = []\n",
        "        for i in range(num_segments):\n",
        "            start = i * segment_size\n",
        "            end = (i + 1) * segment_size if i < num_segments - 1 else len(df_resampled)\n",
        "            segments_args.append((start, end, i + 1, df_resampled)) # Pass df_resampled to avoid global access in pool\n",
        "\n",
        "        # Use multiprocessing to perform FT on segments for efficiency\n",
        "        print(f\"Performing FT on {num_segments} segments using multiprocessing...\")\n",
        "        # Pass df_resampled directly as an argument to process_data_segment\n",
        "        with Pool(processes=cpu_count()) as pool:\n",
        "            segment_fts = pool.map(process_data_segment, segments_args)\n",
        "\n",
        "        # Combine all segment FTs\n",
        "        all_segment_fts = pd.concat(segment_fts, ignore_index=True)\n",
        "\n",
        "        # To create a 'final_output' similar to the original notebook's intent,\n",
        "        # we need to align the full FT and segment FTs. This implies a row-wise concatenation,\n",
        "        # which means that the number of rows in `ft_full_optimal` and `all_segment_fts`\n",
        "        # needs to be the same, which they won't be unless the segment_fts are somehow aggregated.\n",
        "        # The original notebook's concatenation would have produced a DataFrame with many NaN values\n",
        "        # if the shapes didn't align or were not meaningful for concatenation.\n",
        "        # For clarity and correct analysis, we will focus on the two main outputs:\n",
        "        # 1. `ft_full_optimal`: The FT of the entire signal (optimal basis waves).\n",
        "        # 2. `all_segment_fts`: The FTs of all individual segments.\n",
        "\n",
        "        # Saving these two key outputs separately is more robust and meaningful.\n",
        "        ft_full_optimal.to_csv('/content/fourier_transform_full_optimal.csv', index=False)\n",
        "        all_segment_fts.to_csv('/content/fourier_transform_segments.csv', index=False)\n",
        "\n",
        "        print(\"Processing complete. Fourier Transform results for full data and segments saved.\")\n",
        "        print(\"\\nShape of full FT optimal DataFrame:\", ft_full_optimal.shape)\n",
        "        print(\"\\nShape of all segments FT DataFrame:\", all_segment_fts.shape)\n",
        "        print(\"\\nColumns in full FT optimal DataFrame:\", ft_full_optimal.columns)\n",
        "        print(\"\\nColumns in all segments FT DataFrame:\", all_segment_fts.columns)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during segmentation and FT analysis: {e}\")\n",
        "else:\n",
        "    print(\"Cannot perform segmentation and FT analysis as 'df_resampled' is empty.\")\n"
      ],
      "metadata": {
        "id": "v11hbXcNk4iK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Visualizing Segment Boundaries\n",
        "\n",
        "To better understand the segmentation, we visually mark the boundaries of each segment on the original time series chart. This helps in correlating specific time periods with the Fourier Transform results of their respective segments.\n",
        "\n",
        "**Why it's useful:** This visualization provides a clear reference for the time intervals corresponding to each segment's spectral analysis, allowing for a more intuitive interpretation of how market dynamics change over time."
      ],
      "metadata": {
        "id": "8zIDJb95noAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not df_resampled.empty:\n",
        "    try:\n",
        "        # Calculate the slice boundaries\n",
        "        slice_size = len(df_resampled) // 20\n",
        "        slice_boundaries = [df_resampled['Date'].iloc[i * slice_size] for i in range(1, 20)]\n",
        "        slice_boundaries.append(df_resampled['Date'].iloc[-1])  # Add the last date\n",
        "\n",
        "        # Create the plot\n",
        "        plt.figure(figsize=(20, 10))\n",
        "\n",
        "        # Plot the original price data\n",
        "        plt.plot(df_resampled['Date'], df_resampled['EUR/USD'], color='blue', linewidth=0.5, label='EUR/USD')\n",
        "\n",
        "        # Add red lines for each slice\n",
        "        for boundary in slice_boundaries:\n",
        "            plt.axvline(x=boundary, color='red', linestyle='--', linewidth=0.5)\n",
        "\n",
        "        # Customize the plot\n",
        "        plt.title('EUR/USD Exchange Rate Over Time with Segment Markers', fontsize=16)\n",
        "        plt.xlabel('Year', fontsize=12)\n",
        "        plt.ylabel('EUR/USD Exchange Rate', fontsize=12)\n",
        "\n",
        "        # Format x-axis to show years\n",
        "        years = mdates.YearLocator()\n",
        "        years_fmt = mdates.DateFormatter('%Y')\n",
        "        plt.gca().xaxis.set_major_locator(years)\n",
        "        plt.gca().xaxis.set_major_formatter(years_fmt)\n",
        "\n",
        "        # Rotate and align the tick labels so they look better\n",
        "        plt.gcf().autofmt_xdate()\n",
        "\n",
        "        # Add a legend\n",
        "        plt.legend(['EUR/USD', 'Segment Boundaries'], loc='upper left')\n",
        "\n",
        "        # Add grid for better readability\n",
        "        plt.grid(True, linestyle=':', alpha=0.6)\n",
        "\n",
        "        # Save the plot\n",
        "        plt.savefig('/content/forex_timeseries_with_slices.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        print(\"Chart has been saved as 'forex_timeseries_with_slices.png'\")\n",
        "\n",
        "        # Display some statistics about the slices\n",
        "        print(\"\\nSegment Information:\")\n",
        "        for i, boundary in enumerate(slice_boundaries):\n",
        "            start_date = df_resampled['Date'].iloc[i * slice_size] if i * slice_size < len(df_resampled) else 'N/A'\n",
        "            end_date = boundary\n",
        "            print(f\"Segment {i+1}: {start_date} to {end_date}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error visually charting segments: {e}\")\n",
        "else:\n",
        "    print(\"Cannot chart segments as 'df_resampled' is empty.\")\n"
      ],
      "metadata": {
        "id": "5oCuRZ-9l0iE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Comparing Segment-Specific Basis Waves to the Overall Signal\n",
        "\n",
        "This section aims to quantify the similarity between the dominant oscillatory patterns (basis waves) found in each time segment and the overall dominant patterns of the entire EUR/USD signal. We achieve this by:\n",
        "\n",
        "1.  **Extracting Optimal Basis Waves:** Identifying the 20 most significant frequency components from the Fourier Transform of the *entire* dataset. This set represents the 'average' or 'most important' cycles across the full timeframe.\n",
        "2.  **Segment-wise Transform:** Performing Fourier Transform on each individual segment to get its unique set of dominant frequencies, amplitudes, and phases.\n",
        "3.  **Variance Calculation:** Measuring the variance (or squared difference) between the corresponding optimal basis waves of the full signal and those of each segment. Low variance indicates a high degree of similarity, suggesting that the segment's behavior closely aligns with the overall market rhythm.\n",
        "\n",
        "**Why it's useful:** This comparison helps us understand how consistent the market's behavior is over time. Segments with high variance might indicate periods of anomalous or unique market conditions, while segments with low variance suggest periods where the market adheres closely to its long-term characteristics. This information is critical for identifying specific periods conducive to certain trading strategies or for detecting shifts in market regimes."
      ],
      "metadata": {
        "id": "qq74t1nUngNv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_top_n_ft_components(ft_data, n=20):\n",
        "    \"\"\"Extracts the top N most significant Fourier Transform components (excluding DC).\n",
        "\n",
        "    Args:\n",
        "        ft_data (pd.DataFrame): DataFrame from perform_fourier_transform.\n",
        "        n (int): Number of top components to extract.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (frequencies, magnitudes, phases) of the top N components.\n",
        "        float: The DC component (mean).\n",
        "    \"\"\"\n",
        "    # Make a copy to avoid modifying the original DataFrame or its underlying data\n",
        "    temp_ft_data = ft_data.copy()\n",
        "\n",
        "    # Store the DC component (Frequency = 0)\n",
        "    dc_component_row = temp_ft_data[temp_ft_data['Frequency'] == 0]\n",
        "    dc_amplitude = dc_component_row['Amplitude'].iloc[0] if not dc_component_row.empty else 0\n",
        "\n",
        "    # Temporarily set DC component amplitude to 0 for sorting non-DC significant components\n",
        "    temp_ft_data.loc[temp_ft_data['Frequency'] == 0, 'Amplitude'] = 0\n",
        "\n",
        "    # Select the N most significant frequencies (based on amplitude)\n",
        "    top_n_indices = temp_ft_data['Amplitude'].nlargest(n).index\n",
        "    top_n_components = temp_ft_data.loc[top_n_indices]\n",
        "\n",
        "    # Ensure consistency in output length by padding with zeros if not enough components\n",
        "    if len(top_n_components) < n:\n",
        "        missing_rows = n - len(top_n_components)\n",
        "        pad_df = pd.DataFrame(0, index=range(missing_rows), columns=top_n_components.columns)\n",
        "        top_n_components = pd.concat([top_n_components, pad_df], ignore_index=True)\n",
        "\n",
        "    return (\n",
        "        top_n_components['Frequency'].values[:n],\n",
        "        top_n_components['Amplitude'].values[:n],\n",
        "        top_n_components['Phase'].values[:n],\n",
        "        dc_amplitude\n",
        "    )\n",
        "\n",
        "def calculate_transform_variance(optimal_freqs, optimal_mags, optimal_phases,\n",
        "                                 segment_freqs, segment_mags, segment_phases):\n",
        "    \"\"\"Calculates variance between optimal and segment transform components.\n",
        "\n",
        "    Args:\n",
        "        optimal_freqs (np.ndarray): Frequencies from the full signal.\n",
        "        optimal_mags (np.ndarray): Magnitudes from the full signal.\n",
        "        optimal_phases (np.ndarray): Phases from the full signal.\n",
        "        segment_freqs (np.ndarray): Frequencies from the segment.\n",
        "        segment_mags (np.ndarray): Magnitudes from the segment.\n",
        "        segment_phases (np.ndarray): Phases from the segment.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary of variance metrics.\n",
        "    \"\"\"\n",
        "    # Ensure arrays have the same length for direct comparison by truncating to the smaller size\n",
        "    min_len = min(len(optimal_freqs), len(segment_freqs))\n",
        "\n",
        "    freq_var = np.var(optimal_freqs[:min_len] - segment_freqs[:min_len])\n",
        "    mag_var = np.var(optimal_mags[:min_len] - segment_mags[:min_len])\n",
        "    phase_var = np.var(optimal_phases[:min_len] - segment_phases[:min_len])\n",
        "\n",
        "    avg_var = (freq_var + mag_var + phase_var) / 3\n",
        "\n",
        "    return {\n",
        "        'Frequency_Variance': freq_var,\n",
        "        'Magnitude_Variance': mag_var,\n",
        "        'Phase_Variance': phase_var,\n",
        "        'Average_Variance': avg_var\n",
        "    }\n",
        "\n",
        "if 'ft_full_optimal' in locals() and not ft_full_optimal.empty and \\\n",
        "   'all_segment_fts' in locals() and not all_segment_fts.empty:\n",
        "    try:\n",
        "        # Extract optimal basis waves from the full signal (excluding DC component)\n",
        "        optimal_frequencies, optimal_magnitudes, optimal_phases, dc_amplitude = \\\n",
        "            extract_top_n_ft_components(ft_full_optimal.copy(), n=20) # n=20 is a hyperparameter for analysis\n",
        "\n",
        "        results_variance = []\n",
        "        num_segments = 20 # Already defined in previous cell\n",
        "        segment_size = len(df_resampled) // num_segments\n",
        "\n",
        "        for i in range(num_segments):\n",
        "            segment_num = i + 1\n",
        "            start_idx = i * segment_size\n",
        "            end_idx = start_idx + segment_size if i < num_segments - 1 else len(df_resampled)\n",
        "\n",
        "            # Extract segment data for FFT calculation\n",
        "            segment_data = df_resampled['EUR/USD'].iloc[start_idx:end_idx]\n",
        "\n",
        "            # Perform FT on the segment\n",
        "            ft_segment_df, _ = perform_fourier_transform(segment_data.reset_index(drop=True))\n",
        "\n",
        "            # Extract top N components from the segment's FT\n",
        "            segment_frequencies, segment_magnitudes, segment_phases, _ = \\\n",
        "                extract_top_n_ft_components(ft_segment_df.copy(), n=20)\n",
        "\n",
        "            # Calculate variance metrics\n",
        "            variance_metrics = calculate_transform_variance(\n",
        "                optimal_frequencies, optimal_magnitudes, optimal_phases,\n",
        "                segment_frequencies, segment_magnitudes, segment_phases\n",
        "            )\n",
        "\n",
        "            results_variance.append({\n",
        "                'Segment': segment_num,\n",
        "                'Start_Date': df_resampled['Date'].iloc[start_idx],\n",
        "                'End_Date': df_resampled['Date'].iloc[end_idx - 1] if end_idx > 0 else 'N/A',\n",
        "                **variance_metrics\n",
        "            })\n",
        "\n",
        "        results_variance_df = pd.DataFrame(results_variance)\n",
        "        results_variance_df = results_variance_df.sort_values('Average_Variance')\n",
        "\n",
        "        results_variance_df.to_csv('/content/segment_variance_results.csv', index=False)\n",
        "\n",
        "        print(\"Analysis complete. Results saved to 'segment_variance_results.csv'\")\n",
        "        print(\"\\nSummary Statistics of Segment Variance:\")\n",
        "        print(results_variance_df.describe())\n",
        "        print(\"\\nTop 5 best matching segments (least variance to optimal basis waves):\")\n",
        "        print(results_variance_df.head())\n",
        "        print(\"\\nTop 5 worst matching segments (most variance to optimal basis waves):\")\n",
        "        print(results_variance_df.tail())\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in segment variance analysis: {e}\")\n",
        "else:\n",
        "    print(\"Cannot perform segment variance analysis as necessary DataFrames are empty.\")\n"
      ],
      "metadata": {
        "id": "PMOdsQqHmu8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Flattening the Reconstructed Wave and Isolating Oscillations\n",
        "\n",
        "The reconstructed signal, built from the most significant Fourier components, captures the primary trends and strong cycles of the EUR/USD data. To specifically analyze the short-term fluctuations or 'oscillations' around this trend, we perform a 'flattening' transformation. This involves subtracting the reconstructed signal (which represents the underlying trend/major cycles) from the original signal. The result is a new series that ideally has a near-zero mean, representing the deviations from the dominant patterns.\n",
        "\n",
        "**Why it's useful:** This process effectively filters out the main structural movements in the data, allowing us to focus solely on the 'noise' or 'residual' patterns. For investment applications, these isolated oscillations can be highly informative: they might represent short-term mean-reversion opportunities or high-frequency trading signals that are obscured by larger trends. The near-zero mean of these oscillations indicates a balanced distribution of price movements above and below the reconstructed trend, suggesting that the exchange rate is just as likely to revert upwards as downwards to its reconstructed 'equilibrium' over time."
      ],
      "metadata": {
        "id": "2Ax2RzSwx6NN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not df_resampled.empty and 'ft_full_optimal' in locals() and not ft_full_optimal.empty:\n",
        "    try:\n",
        "        n = len(df_resampled)\n",
        "        prices = df_resampled['EUR/USD'].values\n",
        "\n",
        "        # Reconstruct the signal using the top 20 significant frequencies from the full FT\n",
        "        # First, ensure we have the full FFT values corresponding to ft_full_optimal\n",
        "        _, full_fft_raw = perform_fourier_transform(df_resampled.set_index('Date')['EUR/USD'])\n",
        "\n",
        "        # Extract top 20 indices (excluding DC) from the full FFT\n",
        "        # Store the DC component separately\n",
        "        dc_component = full_fft_raw[0]\n",
        "        full_fft_raw_no_dc = full_fft_raw.copy()\n",
        "        full_fft_raw_no_dc[0] = 0 # Temporarily zero out DC for sorting\n",
        "\n",
        "        significance = np.abs(full_fft_raw_no_dc)\n",
        "        top_20_indices = np.argsort(significance)[-20:]\n",
        "        optimal_fft_values = np.zeros(n, dtype=complex)\n",
        "        optimal_fft_values[top_20_indices] = full_fft_raw[top_20_indices] # Use original values for top 20\n",
        "\n",
        "        # Add the DC component back into the reconstruction\n",
        "        optimal_fft_values[0] = dc_component\n",
        "\n",
        "        optimal_reconstruction = np.real(ifft(optimal_fft_values))\n",
        "\n",
        "        # Plot the original vs reconstructed signal\n",
        "        plt.figure(figsize=(20, 10))\n",
        "        plt.plot(df_resampled['Date'], prices, label='Original', alpha=0.7)\n",
        "        plt.plot(df_resampled['Date'], optimal_reconstruction, label='Reconstructed (Top 20 Waves)', alpha=0.7)\n",
        "        plt.title('Original vs Reconstructed EUR/USD Exchange Rate')\n",
        "        plt.xlabel('Date')\n",
        "        plt.ylabel('EUR/USD')\n",
        "        plt.legend()\n",
        "        plt.savefig('/content/original_vs_reconstructed.png')\n",
        "        plt.show()\n",
        "        print(\"Original vs Reconstructed plot saved as 'original_vs_reconstructed.png'\")\n",
        "\n",
        "        # Calculate the transformation to flatten the reconstructed wave\n",
        "        # We subtract the mean of the reconstructed signal to center it around zero\n",
        "        flattening_transform = optimal_reconstruction - np.mean(optimal_reconstruction)\n",
        "\n",
        "        # Apply the transformation to both the reconstructed and original waves\n",
        "        # flattened_reconstruction should now be essentially zero\n",
        "        flattened_reconstruction = optimal_reconstruction - flattening_transform\n",
        "        flattened_original = prices - flattening_transform\n",
        "\n",
        "        # Plot the flattened waves\n",
        "        plt.figure(figsize=(20, 10))\n",
        "        plt.plot(df_resampled['Date'], flattened_original, label='Transformed Original (Oscillations)', alpha=0.7)\n",
        "        plt.plot(df_resampled['Date'], flattened_reconstruction, label='Flattened Reconstruction (Near Zero)', alpha=0.7, linestyle='--')\n",
        "        plt.axhline(y=0, color='r', linestyle='--', label='X-axis')\n",
        "        plt.title('Flattened EUR/USD Exchange Rate (Oscillations Isolated)')\n",
        "        plt.xlabel('Date')\n",
        "        plt.ylabel('Transformed EUR/USD')\n",
        "        plt.legend()\n",
        "        plt.savefig('/content/flattened_waves.png')\n",
        "        plt.show()\n",
        "        print(\"Flattened waves plot saved as 'flattened_waves.png'\")\n",
        "\n",
        "        # Calculate statistics about the flattened waves\n",
        "        original_range = np.ptp(flattened_original)\n",
        "        original_std = np.std(flattened_original)\n",
        "        reconstruction_range = np.ptp(flattened_reconstruction)\n",
        "        reconstruction_std = np.std(flattened_reconstruction)\n",
        "\n",
        "        print(\"\\nStatistics of the flattened waves:\")\n",
        "        print(f\"Original (Oscillations) - Range: {original_range:.6f}, Standard Deviation: {original_std:.6f}\")\n",
        "        print(f\"Reconstruction (Flattened) - Range: {reconstruction_range:.6f}, Standard Deviation: {reconstruction_std:.6f}\")\n",
        "\n",
        "        # Create a DataFrame with the results for later analysis\n",
        "        eur_usd_analysis_results_df = pd.DataFrame({\n",
        "            'Date': df_resampled['Date'],\n",
        "            'Original_Price': prices,\n",
        "            'Reconstructed_Price': optimal_reconstruction,\n",
        "            'Flattened_Original': flattened_original,\n",
        "            'Flattened_Reconstruction': flattened_reconstruction\n",
        "        })\n",
        "\n",
        "        eur_usd_analysis_results_df.to_csv('/content/eur_usd_analysis_results.csv', index=False)\n",
        "        print(\"\\nNumerical results saved to 'eur_usd_analysis_results.csv'\")\n",
        "\n",
        "        # Save statistics to a separate CSV\n",
        "        stats_df = pd.DataFrame({\n",
        "            'Metric': ['Range', 'Standard Deviation'],\n",
        "            'Flattened_Original': [original_range, original_std],\n",
        "            'Flattened_Reconstruction': [reconstruction_range, reconstruction_std]\n",
        "        })\n",
        "        stats_df.to_csv('/content/eur_usd_analysis_stats.csv', index=False)\n",
        "        print(\"Statistics saved to 'eur_usd_analysis_stats.csv'\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during signal flattening: {e}\")\n",
        "else:\n",
        "    print(\"Cannot flatten signal as 'df_resampled' or 'ft_full_optimal' is empty.\")\n"
      ],
      "metadata": {
        "id": "1v2amMqiyBAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Analyzing Oscillations: Autocorrelation and Cyclical Behavior\n",
        "\n",
        "With the primary trends removed, we can now analyze the isolated oscillations for any underlying cyclical patterns or persistence. Autocorrelation is a key statistical tool that measures how a time series is correlated with a lagged version of itself. A high autocorrelation at a specific lag indicates a repeating pattern or cycle. We will also perform spectral analysis to identify dominant frequencies within these oscillations.\n",
        "\n",
        "**Why it's useful:** Understanding the autocorrelation and spectral properties of these oscillations helps determine their predictability. If strong, statistically significant cycles are present, they could be exploited for short-term trading strategies. However, the absence of clear peaks might suggest a more complex, 'long-memory' process that requires advanced machine learning techniques, such as ARIMA or GARCH models, for accurate forecasting."
      ],
      "metadata": {
        "id": "8UMFXtRV033m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'eur_usd_analysis_results_df' in locals() and not eur_usd_analysis_results_df.empty:\n",
        "    try:\n",
        "        # Calculate the oscillations (difference between original and reconstruction)\n",
        "        # This column was already created in the previous step as 'Flattened_Original'\n",
        "        # For clarity, let's refer to it as 'Oscillations' in this context.\n",
        "        oscillations = eur_usd_analysis_results_df['Flattened_Original']\n",
        "\n",
        "        # 1. Basic Statistical Analysis of Oscillations\n",
        "        print(\"\\nBasic Statistical Analysis of Oscillations (Flattened Original):\")\n",
        "        print(oscillations.describe())\n",
        "\n",
        "        # 2. Time Series Plot of Oscillations\n",
        "        plt.figure(figsize=(20, 10))\n",
        "        plt.plot(eur_usd_analysis_results_df['Date'], oscillations, label='Oscillations')\n",
        "        plt.title('Oscillations of Original Signal around Flattened Reconstruction')\n",
        "        plt.xlabel('Date')\n",
        "        plt.ylabel('Oscillation Magnitude')\n",
        "        plt.grid(True, linestyle=':', alpha=0.6)\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('/content/oscillations_time_series.png')\n",
        "        plt.show()\n",
        "        print(\"Oscillations time series plot saved as 'oscillations_time_series.png'\")\n",
        "\n",
        "        # 3. Histogram of Oscillations\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.hist(oscillations, bins=50, edgecolor='black', alpha=0.7)\n",
        "        plt.title('Distribution of Oscillations')\n",
        "        plt.xlabel('Oscillation Magnitude')\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.grid(True, linestyle=':', alpha=0.6)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('/content/oscillations_histogram.png')\n",
        "        plt.show()\n",
        "        print(\"Oscillations histogram plot saved as 'oscillations_histogram.png'\")\n",
        "\n",
        "        # 4. Autocorrelation Analysis\n",
        "        lags_to_plot = 100 # Adjust as needed\n",
        "        autocorr_values = acf(oscillations, nlags=lags_to_plot) # Compute autocorrelation function\n",
        "\n",
        "        plt.figure(figsize=(15, 7))\n",
        "        plot_acf(oscillations, lags=lags_to_plot, ax=plt.gca(), title='Autocorrelation of EUR/USD Oscillations')\n",
        "\n",
        "        # Add vertical lines at multiples of 27 days\n",
        "        # The original notebook stated a 27-day cycle was proven non-existent by auto-correlation.\n",
        "        # These lines are for visual reference based on prior hypothesis.\n",
        "        for i in range(1, int(lags_to_plot / 27) + 1):\n",
        "            plt.axvline(x=i * 27, color='g', linestyle='--', alpha=0.5, label=f'{i*27} Days (Previous Hypothesis)')\n",
        "\n",
        "        # Only add legend if lines are actually plotted\n",
        "        if int(lags_to_plot / 27) >= 1:\n",
        "            handles, labels = plt.gca().get_legend_handles_labels()\n",
        "            unique_labels = dict(zip(labels, handles)) # Deduplicate labels\n",
        "            plt.legend(unique_labels.values(), unique_labels.keys())\n",
        "\n",
        "        plt.xlabel('Lag (days)')\n",
        "        plt.ylabel('Autocorrelation Coefficient')\n",
        "        plt.grid(True, linestyle=':', alpha=0.6)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('/content/oscillations_autocorrelation.png')\n",
        "        plt.show()\n",
        "        print(\"Autocorrelation plot saved as 'oscillations_autocorrelation.png'\")\n",
        "\n",
        "        print(\"\\nAutocorrelation values around 27 days (Previous Hypothesis Reference):\")\n",
        "        for i in range(25, 30): # Checking around the 27-day mark\n",
        "            if i < len(autocorr_values):\n",
        "                print(f\"Lag {i}: {autocorr_values[i]:.4f}\")\n",
        "\n",
        "        # 5. Power Spectral Density (PSD) Analysis\n",
        "        # PSD helps identify the frequencies at which the signal power is concentrated.\n",
        "        f, Pxx = signal.periodogram(oscillations)\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.semilogy(f, Pxx)\n",
        "        plt.title('Power Spectral Density of Oscillations')\n",
        "        plt.xlabel('Frequency (Cycles/Day)')\n",
        "        plt.ylabel('Power (dB)')\n",
        "        plt.grid(True, linestyle=':', alpha=0.6)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('/content/oscillations_psd.png')\n",
        "        plt.show()\n",
        "        print(\"Power Spectral Density plot saved as 'oscillations_psd.png'\")\n",
        "\n",
        "        # 6. Rolling Statistics of Oscillations\n",
        "        # Rolling mean and standard deviation help identify changes in the central tendency and volatility of oscillations over time.\n",
        "        window = 30  # 30-day window is a common choice for monthly trends.\n",
        "        eur_usd_analysis_results_df['Rolling_Mean'] = oscillations.rolling(window=window).mean()\n",
        "        eur_usd_analysis_results_df['Rolling_Std'] = oscillations.rolling(window=window).std()\n",
        "\n",
        "        plt.figure(figsize=(20, 10))\n",
        "        plt.plot(eur_usd_analysis_results_df['Date'], oscillations, label='Oscillations', alpha=0.7)\n",
        "        plt.plot(eur_usd_analysis_results_df['Date'], eur_usd_analysis_results_df['Rolling_Mean'], label=f'{window}-Day Rolling Mean of Oscillations', color='orange')\n",
        "        plt.plot(eur_usd_analysis_results_df['Date'], eur_usd_analysis_results_df['Rolling_Std'], label=f'{window}-Day Rolling Std Dev of Oscillations', color='green')\n",
        "        plt.title(f'Rolling Statistics (Mean and Standard Deviation) of EUR/USD Oscillations (Window={window} Days)')\n",
        "        plt.xlabel('Date')\n",
        "        plt.ylabel('Magnitude')\n",
        "        plt.grid(True, linestyle=':', alpha=0.6)\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('/content/oscillations_rolling_stats.png')\n",
        "        plt.show()\n",
        "        print(\"Rolling statistics plot saved as 'oscillations_rolling_stats.png'\")\n",
        "\n",
        "        # 7. Identifying Potential Cycles (Peaks)\n",
        "        # We identify peaks in the oscillation signal to calculate average distances between them.\n",
        "        # A minimum distance of 20 days is set to avoid capturing very high-frequency noise as distinct peaks.\n",
        "        peaks, _ = find_peaks(oscillations, distance=20)\n",
        "\n",
        "        plt.figure(figsize=(20, 10))\n",
        "        plt.plot(eur_usd_analysis_results_df['Date'], oscillations, label='Oscillations')\n",
        "        plt.plot(eur_usd_analysis_results_df['Date'].iloc[peaks], oscillations.iloc[peaks], \"x\", color='red', markersize=8, label='Identified Peaks')\n",
        "        plt.title('Identified Peaks in EUR/USD Oscillations')\n",
        "        plt.xlabel('Date')\n",
        "        plt.ylabel('Oscillation Magnitude')\n",
        "        plt.grid(True, linestyle=':', alpha=0.6)\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('/content/oscillations_peaks.png')\n",
        "        plt.show()\n",
        "        print(\"Identified peaks plot saved as 'oscillations_peaks.png'\")\n",
        "\n",
        "        # Calculate average distance between peaks\n",
        "        peak_distances = np.diff(peaks)\n",
        "        if len(peak_distances) > 0:\n",
        "            avg_peak_distance = np.mean(peak_distances)\n",
        "            print(f\"\\nAverage distance between peaks: {avg_peak_distance:.2f} days\")\n",
        "        else:\n",
        "            print(\"\\nNo sufficient peaks found to calculate average distance.\")\n",
        "\n",
        "        # Save the full oscillation analysis results\n",
        "        eur_usd_analysis_results_df.to_csv('/content/eur_usd_oscillation_analysis.csv', index=False)\n",
        "        print(\"\\nOscillation analysis results saved to 'eur_usd_oscillation_analysis.csv'\")\n",
        "        print(\"All analysis plots have been saved as PNG files in the current directory.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during oscillation analysis: {e}\")\n",
        "else:\n",
        "    print(\"Cannot perform oscillation analysis as 'eur_usd_analysis_results_df' is empty.\")\n"
      ],
      "metadata": {
        "id": "qlWAsNJizCCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Confirming Mean Distribution of Flattened Signal\n",
        "\n",
        "After flattening the signal by subtracting the reconstructed trend, it's essential to verify that the resulting oscillations truly have a near-zero mean. This confirms that the trend has been effectively removed and that the oscillations are indeed distributed symmetrically around zero. This is a critical validation step to ensure the integrity of the isolated oscillatory component.\n",
        "\n",
        "**Why it's useful:** A near-zero mean in the oscillations indicates that positive and negative deviations from the trend are approximately balanced. This is a characteristic feature of 'noise' or 'residual' components in time series and suggests that the original signal is just as likely to move above its reconstructed 'equilibrium' as below it. This balanced nature is valuable for identifying mean-reversion trading opportunities, where the price tends to return to its average over time."
      ],
      "metadata": {
        "id": "CDI_6wNy5DMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'eur_usd_analysis_results_df' in locals() and not eur_usd_analysis_results_df.empty:\n",
        "    try:\n",
        "        # The 'Flattened_Reconstruction' column should be near zero, and 'Flattened_Original' represents the oscillations\n",
        "        # The original notebook's intent was to check: `Reconstructed_Price - Flattened_Reconstruction`\n",
        "        # which effectively checks how close `Flattened_Reconstruction` is to `Reconstructed_Price`.\n",
        "        # However, `Flattened_Reconstruction` should already be close to zero if `optimal_reconstruction - np.mean(optimal_reconstruction)` was the target.\n",
        "        # Let's clarify: if 'Flattened_Reconstruction' is the reconstructed wave *minus its own mean* (as per code),\n",
        "        # then its mean should be near zero.\n",
        "        # The user's prompt indicated 'Reconstructed_Price - Flattened_Reconstruction' as the difference.\n",
        "        # If `Flattened_Reconstruction` is the *original price* minus the *flattening_transform*,\n",
        "        # then `Flattened_Reconstruction` is effectively the *mean of the reconstructed price*.\n",
        "        # Let's re-verify the definition from the previous cell:\n",
        "        # `flattened_reconstruction = optimal_reconstruction - flattening_transform`\n",
        "        # `flattening_transform = optimal_reconstruction - np.mean(optimal_reconstruction)`\n",
        "        # So, `flattened_reconstruction = optimal_reconstruction - (optimal_reconstruction - np.mean(optimal_reconstruction))`\n",
        "        # `flattened_reconstruction = np.mean(optimal_reconstruction)` (a constant value)\n",
        "\n",
        "        # This means the 'Flattened_Reconstruction' is indeed a constant: the mean of the original optimal reconstruction.\n",
        "        # The actual oscillations are in 'Flattened_Original' because `flattened_original = prices - flattening_transform`.\n",
        "        # And `flattening_transform` effectively centers `optimal_reconstruction` at zero.\n",
        "\n",
        "        # So, the correct data to check the mean of oscillations is 'Flattened_Original'.\n",
        "        oscillations_to_verify = eur_usd_analysis_results_df['Flattened_Original']\n",
        "\n",
        "        total_sum_oscillations = np.sum(oscillations_to_verify)\n",
        "        mean_oscillations = np.mean(oscillations_to_verify)\n",
        "        std_oscillations = np.std(oscillations_to_verify)\n",
        "\n",
        "        print(f\"\\nVerifying the mean of the isolated oscillations ('Flattened_Original'):\")\n",
        "        print(f\"Sum of oscillations: {total_sum_oscillations}\")\n",
        "        print(f\"Number of data points: {len(oscillations_to_verify)}\")\n",
        "        print(f\"Mean of the oscillations: {mean_oscillations}\")\n",
        "        print(f\"Standard deviation of the oscillations: {std_oscillations}\")\n",
        "\n",
        "        # Also confirm the mean of the 'Flattened_Reconstruction' is indeed the mean of the original reconstruction\n",
        "        constant_flattened_reconstruction = eur_usd_analysis_results_df['Flattened_Reconstruction'].iloc[0]\n",
        "        original_reconstruction_mean = np.mean(eur_usd_analysis_results_df['Reconstructed_Price'])\n",
        "        print(f\"\\nValue of 'Flattened_Reconstruction' (should be constant): {constant_flattened_reconstruction:.6f}\")\n",
        "        print(f\"Mean of original 'Reconstructed_Price': {original_reconstruction_mean:.6f}\")\n",
        "        print(\"This confirms 'Flattened_Reconstruction' holds the mean of the 'Reconstructed_Price', effectively shifting the original data down by that mean for oscillation analysis.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during mean distribution confirmation: {e}\")\n",
        "else:\n",
        "    print(\"Cannot confirm mean distribution as 'eur_usd_analysis_results_df' is empty.\")\n"
      ],
      "metadata": {
        "id": "P-bKYS6a5FN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. Detecting Aggressive Price Movements and Segment Analysis\n",
        "\n",
        "Identifying 'aggressive' price movements (periods of unusually high volatility or large returns) is crucial for risk management and opportunistic trading. We detect these movements by comparing daily returns to a rolling standard deviation, flagging instances where returns exceed a certain threshold (e.g., 2 standard deviations). Subsequently, we analyze the characteristics of the Fourier and Wavelet Transforms around these aggressive movements.\n",
        "\n",
        "**Why it's useful:** Understanding the underlying frequency and temporal characteristics of aggressive movements can help predict their onset or inform trading strategies during volatile periods. Comparing Wavelet and Fourier Transforms in these specific contexts allows us to investigate which transform provides a more stable or informative representation during high-impact events. This analysis is intended to analyze their use in this specific case, aiming to determine if there is enough signal in the data set to apply advanced ML techniques."
      ],
      "metadata": {
        "id": "_iSckuM2oe4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_aggressive_movements(returns, window=20, threshold=2):\n",
        "    \"\"\"Detects aggressive price movements based on rolling standard deviation.\n",
        "\n",
        "    Args:\n",
        "        returns (pd.Series): Time series of financial returns.\n",
        "        window (int): Rolling window size for standard deviation calculation.\n",
        "        threshold (int): Multiplier for standard deviation to set the aggressive movement threshold.\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: Boolean Series indicating aggressive movements.\n",
        "    \"\"\"\n",
        "    rolling_std = returns.rolling(window=window).std()\n",
        "    # Aggressive moves are where absolute returns exceed the threshold times rolling std\n",
        "    aggressive_moves = np.abs(returns) > (threshold * rolling_std)\n",
        "    return aggressive_moves\n",
        "\n",
        "def wavelet_transform_segment(data_array, wavelet='db4', level=None):\n",
        "    \"\"\"Performs Discrete Wavelet Transform on a data segment.\n",
        "\n",
        "    Args:\n",
        "        data_array (np.ndarray): The segment of time series data.\n",
        "        wavelet (str): Name of the wavelet to use (e.g., 'db4').\n",
        "        level (int, optional): Decomposition level. Defaults to min(5, max_level).\n",
        "\n",
        "    Returns:\n",
        "        list: List of wavelet coefficients.\n",
        "    \"\"\"\n",
        "    if len(data_array) < pywt.dwt_coeff_len(len(data_array), pywt.Wavelet(wavelet).dec_len):\n",
        "        # Pad the data if too short for the chosen wavelet and level\n",
        "        pad_len = pywt.dwt_coeff_len(len(data_array), pywt.Wavelet(wavelet).dec_len) - len(data_array)\n",
        "        data_array = np.pad(data_array, (0, pad_len), 'constant')\n",
        "\n",
        "    # Automatically determine the maximum decomposition level\n",
        "    max_level = pywt.dwt_max_level(len(data_array), wavelet)\n",
        "    actual_level = min(5, max_level) if level is None else min(level, max_level)\n",
        "\n",
        "    # Ensure actual_level is at least 1 if possible\n",
        "    if actual_level == 0 and max_level > 0:\n",
        "        actual_level = 1\n",
        "\n",
        "    coeffs = pywt.wavedec(data_array, wavelet, level=actual_level)\n",
        "    return coeffs\n",
        "\n",
        "def fourier_transform_segment(data_array):\n",
        "    \"\"\"Performs Fourier Transform on a data segment.\n",
        "\n",
        "    Args:\n",
        "        data_array (np.ndarray): The segment of time series data.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Raw FFT values.\n",
        "    \"\"\"\n",
        "    return fft(data_array)\n",
        "\n",
        "def compare_normalized_transforms(wavelet_coeffs, fourier_coeffs):\n",
        "    \"\"\"Compares normalized wavelet and Fourier transforms using correlation.\n",
        "\n",
        "    Args:\n",
        "        wavelet_coeffs (list): List of wavelet coefficients.\n",
        "        fourier_coeffs (np.ndarray): Raw Fourier Transform values.\n",
        "\n",
        "    Returns:\n",
        "        float: Correlation coefficient between the normalized transforms.\n",
        "    \"\"\"\n",
        "    # Flatten and take absolute values for comparison (magnitudes)\n",
        "    # Handle potential empty or single-element arrays from wavedec\n",
        "    if len(wavelet_coeffs) == 0: # Check if wavelet_coeffs is empty\n",
        "        normalized_wavelet = np.array([0.0]) # Use a placeholder to avoid error\n",
        "    else:\n",
        "        # np.concatenate can return a 0-D array if all elements are empty or 0-D\n",
        "        concatenated_wavelet = np.concatenate([c.flatten() for c in wavelet_coeffs if c.size > 0])\n",
        "        if concatenated_wavelet.size == 0:\n",
        "            normalized_wavelet = np.array([0.0])\n",
        "        else:\n",
        "            # Reshape to 2D for StandardScaler\n",
        "            normalized_wavelet = StandardScaler().fit_transform(concatenated_wavelet.reshape(-1, 1)).flatten()\n",
        "\n",
        "    # Ensure fourier_coeffs is 1D and has at least one element\n",
        "    fourier_magnitudes = np.abs(fourier_coeffs).flatten()\n",
        "    if fourier_magnitudes.size == 0:\n",
        "        normalized_fourier = np.array([0.0])\n",
        "    else:\n",
        "        # Reshape to 2D for StandardScaler\n",
        "        normalized_fourier = StandardScaler().fit_transform(fourier_magnitudes.reshape(-1, 1)).flatten()\n",
        "\n",
        "    # Ensure both arrays have the same length for correlation calculation\n",
        "    min_len = min(len(normalized_wavelet), len(normalized_fourier))\n",
        "    if min_len == 0: # If min_len is 0, correlation is undefined, return NaN\n",
        "        return np.nan\n",
        "    elif min_len == 1: # If min_len is 1, correlation is 1 if values are identical, else 0\n",
        "        return 1.0 if np.isclose(normalized_wavelet[0], normalized_fourier[0]) else 0.0\n",
        "    else:\n",
        "        correlation = np.corrcoef(normalized_wavelet[:min_len], normalized_fourier[:min_len])[0, 1]\n",
        "        return correlation\n",
        "\n",
        "if not df_resampled.empty and 'Returns' in df_resampled.columns:\n",
        "    try:\n",
        "        # Detect aggressive movements (adjust window and threshold as needed)\n",
        "        # A threshold of 2 standard deviations is a common statistical heuristic for outliers.\n",
        "        df_resampled['Aggressive_Move'] = detect_aggressive_movements(df_resampled['Returns'], window=20, threshold=2)\n",
        "\n",
        "        # Analyze each aggressive movement\n",
        "        aggressive_movement_results = []\n",
        "        pad = 50  # Padding before and after aggressive movement for context in the segment\n",
        "\n",
        "        num_data_points = len(df_resampled)\n",
        "        num_segments_analysis = 20 # Keep consistent with prior segmentation\n",
        "        segment_len_for_index = num_data_points // num_segments_analysis\n",
        "\n",
        "        # Iterate through the DataFrame to find aggressive moves\n",
        "        for i in range(num_data_points):\n",
        "            if df_resampled['Aggressive_Move'].iloc[i]:\n",
        "                start_segment = max(0, i - pad)\n",
        "                end_segment = min(num_data_points, i + pad + 1)\n",
        "\n",
        "                # Extract data around the aggressive movement\n",
        "                segment_data = df_resampled['EUR/USD'].iloc[start_segment:end_segment].values\n",
        "\n",
        "                # Ensure segment_data is not empty and has enough points for transforms\n",
        "                if len(segment_data) > 1: # FFT/Wavelet need at least 2 points\n",
        "                    try:\n",
        "                        # Perform wavelet transform\n",
        "                        wavelet_coeffs = wavelet_transform_segment(segment_data)\n",
        "\n",
        "                        # Perform Fourier transform\n",
        "                        fourier_coeffs = fourier_transform_segment(segment_data)\n",
        "\n",
        "                        # Compare transforms\n",
        "                        correlation = compare_normalized_transforms(wavelet_coeffs, fourier_coeffs)\n",
        "\n",
        "                        # Find which of the 20 segments this aggressive movement belongs to\n",
        "                        # Handle edge case for the last segment index\n",
        "                        segment_index = min(num_segments_analysis, (i // segment_len_for_index) + 1)\n",
        "\n",
        "                        aggressive_movement_results.append({\n",
        "                            'Date': df_resampled.index[i],\n",
        "                            'Segment': segment_index,\n",
        "                            'Correlation': correlation\n",
        "                        })\n",
        "                    except Exception as transform_e:\n",
        "                        print(f\"Warning: Could not process segment for date {df_resampled.index[i]} due to transform error: {transform_e}\")\n",
        "                else:\n",
        "                    print(f\"Warning: Segment too short for analysis at date {df_resampled.index[i]}\")\n",
        "\n",
        "        # Create DataFrame with results\n",
        "        aggressive_movement_analysis_df = pd.DataFrame(aggressive_movement_results)\n",
        "\n",
        "        # Analyze patterns in each of the 20 broader segments\n",
        "        segment_correlation_analysis = []\n",
        "        for segment_num in range(1, num_segments_analysis + 1):\n",
        "            segment_results = aggressive_movement_analysis_df[aggressive_movement_analysis_df['Segment'] == segment_num]\n",
        "            avg_correlation = segment_results['Correlation'].mean() if not segment_results.empty else np.nan\n",
        "            num_aggressive_movements = len(segment_results)\n",
        "\n",
        "            segment_correlation_analysis.append({\n",
        "                'Segment': segment_num,\n",
        "                'Avg_Correlation': avg_correlation,\n",
        "                'Num_Aggressive_Movements': num_aggressive_movements\n",
        "            })\n",
        "\n",
        "        segment_correlation_analysis_df = pd.DataFrame(segment_correlation_analysis)\n",
        "\n",
        "        # Save results\n",
        "        aggressive_movement_analysis_df.to_csv('/content/aggressive_movement_analysis.csv', index=False)\n",
        "        segment_correlation_analysis_df.to_csv('/content/segment_pattern_analysis.csv', index=False)\n",
        "\n",
        "        # Plot results (Correlation vs Date for aggressive movements)\n",
        "        plt.figure(figsize=(15, 10))\n",
        "        plt.scatter(aggressive_movement_analysis_df['Date'], aggressive_movement_analysis_df['Correlation'], alpha=0.5)\n",
        "        plt.title('Correlation between Wavelet and Fourier Transforms for Aggressive Movements')\n",
        "        plt.xlabel('Date')\n",
        "        plt.ylabel('Correlation')\n",
        "        plt.grid(True, linestyle=':', alpha=0.6)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('/content/correlation_plot.png')\n",
        "        plt.show()\n",
        "        print(\"Analysis complete. Results saved to CSV files and correlation plot generated.\")\n",
        "\n",
        "        # Print summary\n",
        "        print(\"\\nSummary of Aggressive Movements by Segment (Sorted by Number of Movements):\")\n",
        "        print(segment_correlation_analysis_df.sort_values('Num_Aggressive_Movements', ascending=False))\n",
        "\n",
        "        print(\"\\nSegments with lowest average correlation (potential unique patterns during aggressive moves):\")\n",
        "        print(segment_correlation_analysis_df.sort_values('Avg_Correlation').head())\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in aggressive movement detection and analysis: {e}\")\n",
        "else:\n",
        "    print(\"Cannot detect aggressive movements as 'df_resampled' or 'Returns' column is empty.\")\n"
      ],
      "metadata": {
        "id": "aQK8hIDmok8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key Takeaways for our Client\n",
        "\n",
        "This research initiative has provided critical insights into the oscillatory behavior of the EUR/USD exchange rate, forming a robust foundation for applying advanced AI/ML techniques in algorithmic trading strategy development.\n",
        "\n",
        "* **Data Integrity and Preparation:** We successfully processed and resampled one year of minute-level EUR/USD data, ensuring a clean and uniform time series for Fourier Transform analysis. The initial charting revealed macroscopic trends and volatility, setting the stage for deeper spectral decomposition.\n",
        "\n",
        "* **Fourier Transform Reveals Underlying Rhythms:** The Fourier Transform effectively decomposed the price signal into its constituent frequencies, providing valuable information on amplitude and phase. This transformation is fundamental for identifying recurring patterns that could drive price movements. The conceptual visualization on a Bloch Sphere provides a novel way to interpret these complex components, potentially opening doors for quantum computing applications in market analysis.\n",
        "\n",
        "* **Isolation of Oscillations for Targeted Analysis:** By flattening the reconstructed signal (removing the primary trends captured by significant Fourier components), we successfully isolated the short-term oscillations. Statistical validation confirmed these oscillations exhibit a near-zero mean, indicating a balanced distribution around the reconstructed equilibrium. This suggests opportunities for mean-reversion strategies, as the price is as likely to revert upwards as downwards to its trend.\n",
        "\n",
        "* **Complex Cyclical Structure, Not Simple Rhythms:** While initial peak analysis suggested a potential 27-day cycle in oscillations (consistent with monthly economic drivers), further autocorrelation analysis revealed a complex, long-memory structure rather than a simple, dominant fixed-period cycle. This indicates that while past movements do influence future values for an extended period, a straightforward 27-day pattern is not a primary, statistically significant feature. This outcome is crucial as it informs us that simple, fixed-period trading strategies are unlikely to be effective and that more sophisticated, adaptive models will be required.\n",
        "\n",
        "* **Foundation for Advanced ML Techniques:** The analysis confirms that there is sufficient signal in the dataset to apply advanced machine learning techniques, some of which leverage Fourier and Wavelet Transforms. The intricate patterns observed in the oscillations, particularly their long-lasting correlations and the nuanced insights from comparing Fourier and Wavelet transforms during aggressive price movements, highlight the necessity for models capable of discerning complex, non-linear relationships. This work sets the stage for developing cutting-edge predictive models and sophisticated algorithmic trading strategies for your investment firm.\n",
        "\n",
        "apoth3osis R&D remains committed to pioneering solutions at the intersection of AI/ML and financial markets, transforming complex data into actionable insights for your investment success."
      ],
      "metadata": {
        "id": "lBWPUmL_zfyD"
      }
    }
  ]
}